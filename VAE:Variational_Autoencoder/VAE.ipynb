{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import nn\n",
    "\n",
    "from torchsummary import summary\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_dataset = torchvision.datasets.MNIST(\n",
    "    root=\"Mnist_dataset\",\n",
    "    download=True,\n",
    "    train=True,\n",
    "    transform=ToTensor()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train,val,test = torch.utils.data.random_split(mnist_dataset,(0.8,0.1,0.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 500\n",
    "train_loader = DataLoader(\n",
    "    dataset=train,\n",
    "    shuffle=True,\n",
    "    batch_size=batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_loader = DataLoader(\n",
    "    dataset=val,\n",
    "    shuffle=True,\n",
    "    batch_size=batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VAE Encoder\n",
    "Similar to standard Autoencoder, but with two independent lineal layer to represent mean and logvar\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Conv_block(in_channels,out_channels, kernel_size = 4, stride = 2, padding = 1):\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(\n",
    "            in_channels=in_channels,\n",
    "            out_channels=out_channels,\n",
    "            kernel_size=kernel_size,\n",
    "            stride=stride,\n",
    "            padding=padding,\n",
    "        ),\n",
    "        nn.BatchNorm2d(out_channels),\n",
    "        nn.ReLU()\n",
    "    )\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, in_channels:int , latent_dims:int):\n",
    "        super().__init__()\n",
    "        self.convolutions = nn.Sequential(\n",
    "            Conv_block(in_channels,128),\n",
    "            Conv_block(128,256),\n",
    "            Conv_block(256,512),\n",
    "            Conv_block(512,1024)\n",
    "        )\n",
    "\n",
    "        self.mu = nn.Linear(1024,latent_dims)\n",
    "        self.logvar = nn.Linear(1024,latent_dims)\n",
    "\n",
    "    def forward(self, x:torch.Tensor):\n",
    "        bs = x.shape[0]\n",
    "        # print(x.shape)\n",
    "\n",
    "        x = self.convolutions(x)\n",
    "        x = x.reshape(bs,-1)\n",
    "        # x = x.flatten(start_dim=1)\n",
    "        mu = self.mu(x)\n",
    "        logvar = self.logvar(x)\n",
    "\n",
    "        return mu,logvar\n",
    "        # return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [100, 128, 14, 14]           2,176\n",
      "       BatchNorm2d-2         [100, 128, 14, 14]             256\n",
      "              ReLU-3         [100, 128, 14, 14]               0\n",
      "            Conv2d-4           [100, 256, 7, 7]         524,544\n",
      "       BatchNorm2d-5           [100, 256, 7, 7]             512\n",
      "              ReLU-6           [100, 256, 7, 7]               0\n",
      "            Conv2d-7           [100, 512, 3, 3]       2,097,664\n",
      "       BatchNorm2d-8           [100, 512, 3, 3]           1,024\n",
      "              ReLU-9           [100, 512, 3, 3]               0\n",
      "           Conv2d-10          [100, 1024, 1, 1]       8,389,632\n",
      "      BatchNorm2d-11          [100, 1024, 1, 1]           2,048\n",
      "             ReLU-12          [100, 1024, 1, 1]               0\n",
      "           Linear-13                   [100, 2]           2,050\n",
      "           Linear-14                   [100, 2]           2,050\n",
      "================================================================\n",
      "Total params: 11,021,956\n",
      "Trainable params: 11,021,956\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.30\n",
      "Forward/backward pass size (MB): 99.03\n",
      "Params size (MB): 42.05\n",
      "Estimated Total Size (MB): 141.37\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# VAE_encoder = Encoder(in_channels=1,latent_dims=2).cuda()\n",
    "\n",
    "# # VAE_encoder(a.cuda()).shape\n",
    "\n",
    "# summary(VAE_encoder, (1,28,28), batch_size=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder\n",
    "Similar to standard decoder, but take a data from distribution defined by encoder.\n",
    "Actually, we don't need to make non changes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_transpose_block(\n",
    "        in_channels,\n",
    "        out_channels,\n",
    "        kernel_size=3,\n",
    "        stride=2,\n",
    "        padding=1,\n",
    "        output_padding=0,\n",
    "        with_act=True\n",
    "):\n",
    "    modules = [\n",
    "        nn.ConvTranspose2d(\n",
    "            in_channels,\n",
    "            out_channels,\n",
    "            kernel_size=kernel_size,\n",
    "            stride=stride,\n",
    "            padding=padding,\n",
    "            output_padding=output_padding\n",
    "            )   \n",
    "    ]\n",
    "\n",
    "    if with_act:\n",
    "        modules.append(nn.BatchNorm2d(out_channels))\n",
    "        modules.append(nn.ReLU())\n",
    "\n",
    "    return nn.Sequential(*modules)\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self,out_channels:int , latent_dims:int):\n",
    "        super().__init__()\n",
    "\n",
    "        self.lineal = nn.Linear(latent_dims,1024*4*4)\n",
    "        self.t_conv = nn.Sequential(\n",
    "            conv_transpose_block(1024,512),\n",
    "            conv_transpose_block(512,256,output_padding=1),\n",
    "            conv_transpose_block(256,out_channels,output_padding=1,with_act=False)\n",
    "        )\n",
    "\n",
    "    def forward(self, x:torch.Tensor):\n",
    "        bs = x.shape[0]\n",
    "\n",
    "        x = self.lineal(x)\n",
    "        x = x.reshape((bs,1024,4,4))\n",
    "        x = self.t_conv(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variational Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE_AutoEncoder(nn.Module):\n",
    "    def __init__(self, in_channels, latent_dims:int):\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder(in_channels,latent_dims)\n",
    "        self.decoder = Decoder(in_channels,latent_dims)\n",
    "\n",
    "    def encode(self,x):\n",
    "        return self.encoder(x)\n",
    "\n",
    "    def decode(self,z):\n",
    "        return self.decoder(z)\n",
    "\n",
    "\n",
    "    def forward(self,x):\n",
    "        mu,logvar = self.encode(x)\n",
    "        \n",
    "        #Take a sample from distribution\n",
    "        std = torch.exp(0.5*logvar) # Compute standard desviation\n",
    "        z = self.sample(mu,std)\n",
    "        \n",
    "        reconstructed = self.decode(z)\n",
    "\n",
    "        # Return reconstructed image and, mean and logvar to calculate losses\n",
    "        return reconstructed, mu, logvar\n",
    "\n",
    "\n",
    "\n",
    "    def sample(self,mu,std):\n",
    "        standard_sample = torch.randn_like(std)\n",
    "\n",
    "        return mu + (standard_sample * std) # Take a sample of standarized distribution and transforming it to mean and desviation given\n",
    "                                            # This method is used to simplify gradient calculus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1          [-1, 128, 14, 14]           2,176\n",
      "       BatchNorm2d-2          [-1, 128, 14, 14]             256\n",
      "              ReLU-3          [-1, 128, 14, 14]               0\n",
      "            Conv2d-4            [-1, 256, 7, 7]         524,544\n",
      "       BatchNorm2d-5            [-1, 256, 7, 7]             512\n",
      "              ReLU-6            [-1, 256, 7, 7]               0\n",
      "            Conv2d-7            [-1, 512, 3, 3]       2,097,664\n",
      "       BatchNorm2d-8            [-1, 512, 3, 3]           1,024\n",
      "              ReLU-9            [-1, 512, 3, 3]               0\n",
      "           Conv2d-10           [-1, 1024, 1, 1]       8,389,632\n",
      "      BatchNorm2d-11           [-1, 1024, 1, 1]           2,048\n",
      "             ReLU-12           [-1, 1024, 1, 1]               0\n",
      "           Linear-13                    [-1, 2]           2,050\n",
      "           Linear-14                    [-1, 2]           2,050\n",
      "          Encoder-15         [[-1, 2], [-1, 2]]               0\n",
      "           Linear-16                [-1, 16384]          49,152\n",
      "  ConvTranspose2d-17            [-1, 512, 7, 7]       4,719,104\n",
      "      BatchNorm2d-18            [-1, 512, 7, 7]           1,024\n",
      "             ReLU-19            [-1, 512, 7, 7]               0\n",
      "  ConvTranspose2d-20          [-1, 256, 14, 14]       1,179,904\n",
      "      BatchNorm2d-21          [-1, 256, 14, 14]             512\n",
      "             ReLU-22          [-1, 256, 14, 14]               0\n",
      "  ConvTranspose2d-23            [-1, 1, 28, 28]           2,305\n",
      "          Decoder-24            [-1, 1, 28, 28]               0\n",
      "================================================================\n",
      "Total params: 16,973,957\n",
      "Trainable params: 16,973,957\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 2.85\n",
      "Params size (MB): 64.75\n",
      "Estimated Total Size (MB): 67.60\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "VAE_model = VAE_AutoEncoder(in_channels=1,latent_dims=2).cuda()\n",
    "summary(VAE_model,(1,28,28))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training VAE\n",
    "We create a new loss from combitanion of two losses:\n",
    "* The old one, from recreated images\n",
    "* A new from logvar: *Kullback–Leibler divergence* (KLD) is a functions that compares a distribution from another one, in this case, the standard\n",
    "\n",
    "This combination is a sum of boths\n",
    "\n",
    "In some cases, we can put more weigth on one of the lossesx, to balance the reconstruction of gaussian distribution.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "def vae_loss(batch, reconstructed_image, mu, logvar):\n",
    "    bs = batch.shape[0]\n",
    "\n",
    "\n",
    "    reconstruction_loss = F.mse_loss(\n",
    "        reconstructed_image.reshape(bs, -1),\n",
    "        batch.reshape(bs,-1),\n",
    "        reduction=\"none\"\n",
    "    ).sum(dim=-1)\n",
    "\n",
    "    KL_loss = -0.5* torch.sum(1 + logvar - mu.pow(2) - logvar.exp(), dim=-1)\n",
    "\n",
    "    loss = (reconstruction_loss + KL_loss).mean(dim=0)\n",
    "\n",
    "    return (loss, reconstruction_loss, KL_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(loader:torch.utils.data.DataLoader, model : nn.Module, optimizer:torch.optim, loss_fn:callable):\n",
    "    nlotes = len(loader)\n",
    "    # train_size = len(loader.dataset)\n",
    "\n",
    "    model.train()   #Preparo el modelo para el entrenamiento\n",
    "\n",
    "    losses = {\n",
    "        \"loss\": [],\n",
    "        \"reconstruction_loss\": [],\n",
    "        \"KL_loss\": []\n",
    "    }\n",
    "\n",
    "    train_losses = 0\n",
    "\n",
    "    # losses_list = []\n",
    "\n",
    "    for nlote,(x,_) in enumerate(loader):\n",
    "        x = x.cuda()\n",
    "\n",
    "        # Forward Pass\n",
    "        reconstructed ,mu ,logvar = model(x)\n",
    "\n",
    "        # Backpropagation\n",
    "        loss,reconstruction_loss,KL_loss = loss_fn(x, reconstructed, mu, logvar)    #Calculo de loss\n",
    "        \n",
    "        loss.backward()             #Calculo de gradiente\n",
    "        \n",
    "        # Save Losses\n",
    "        losses[\"loss\"].append(\n",
    "            loss.item())\n",
    "        losses[\"reconstruction_loss\"].append(\n",
    "            reconstruction_loss.mean().item()\n",
    "        )\n",
    "        losses[\"KL_loss\"].append(\n",
    "            KL_loss.mean().item()\n",
    "        )\n",
    "\n",
    "        optimizer.step()            #Actualización de parámetros\n",
    "        optimizer.zero_grad()       #Limpieza del optimizador\n",
    "\n",
    "        #Guardamos algunas caractgerísticas para plotear al final\n",
    "        train_losses += loss.item()\n",
    "\n",
    "        #Muestra del proceso\n",
    "        if nlote % 10 == 0:\n",
    "            print(\"Nº de lote:\\t\",nlote)\n",
    "            print(\"Loss:\\t\\t\\t\",loss.item())\n",
    "            print(\"Reconstruction_loss:\\t\",reconstruction_loss.mean().item())\n",
    "            print(\"KL_loss:\\t\\t\",KL_loss.mean().item())\n",
    "            print()\n",
    "\n",
    "    train_losses /= nlotes\n",
    "    print()\n",
    "    print(\"\\tAccuracy/Loss Promedio\")\n",
    "    # print(f\"\\t\\tEntrenamiento: {(100*train_accuracy):>0.1f}% / {train_losses:>8f}\")\n",
    "    print(f\"\\t\\tEntrenamiento: {train_losses:>8f}\")\n",
    "\n",
    "    return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def val_loop(loader:torch.utils.data.DataLoader, model:nn.Module, loss_fn:callable):\n",
    "    \n",
    "    # val_size = len(loader.dataset)\n",
    "    nlotes = len(loader)\n",
    "\n",
    "    model.eval()         #Preparo el modelo para inferencia\n",
    "\n",
    "    val_losses = 0\n",
    "    losses_list = []\n",
    "\n",
    "    with torch.no_grad():       #Calcelo el calculo del gradiente\n",
    "        for x,_ in loader:\n",
    "            \n",
    "            x = x.cuda()\n",
    "            with torch.no_grad():\n",
    "                recosntructed, mu, logvar = model(x)       # Inferencia\n",
    "            \n",
    "            loss,_,_ = loss_fn(x, recosntructed, mu, logvar)\n",
    "\n",
    "            val_losses += loss.item()\n",
    "            losses_list.append(loss.item())\n",
    "\n",
    "    val_losses /= nlotes\n",
    "\n",
    "    print(f\"\\t\\t Validación: {val_losses:>8f}\")\n",
    "\n",
    "    return losses_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "lr = 0.0001\n",
    "\n",
    "optimizer = torch.optim.AdamW(VAE_model.parameters(),lr=lr,eps=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Itenración: 1 / 10 -----------------------------\n",
      "Nº de lote:\t 0\n",
      "Loss:\t\t\t 40.38923645019531\n",
      "Reconstruction_loss:\t 35.760196685791016\n",
      "KL_loss:\t\t 4.62903356552124\n",
      "\n",
      "Nº de lote:\t 10\n",
      "Loss:\t\t\t 42.82951736450195\n",
      "Reconstruction_loss:\t 38.284149169921875\n",
      "KL_loss:\t\t 4.5453715324401855\n",
      "\n",
      "Nº de lote:\t 20\n",
      "Loss:\t\t\t 40.888427734375\n",
      "Reconstruction_loss:\t 36.21786117553711\n",
      "KL_loss:\t\t 4.670566558837891\n",
      "\n",
      "Nº de lote:\t 30\n",
      "Loss:\t\t\t 40.95425796508789\n",
      "Reconstruction_loss:\t 36.30746841430664\n",
      "KL_loss:\t\t 4.646793365478516\n",
      "\n",
      "Nº de lote:\t 40\n",
      "Loss:\t\t\t 41.938846588134766\n",
      "Reconstruction_loss:\t 37.24623489379883\n",
      "KL_loss:\t\t 4.6926093101501465\n",
      "\n",
      "Nº de lote:\t 50\n",
      "Loss:\t\t\t 41.160621643066406\n",
      "Reconstruction_loss:\t 36.57461166381836\n",
      "KL_loss:\t\t 4.5860114097595215\n",
      "\n",
      "Nº de lote:\t 60\n",
      "Loss:\t\t\t 42.45769500732422\n",
      "Reconstruction_loss:\t 37.78338623046875\n",
      "KL_loss:\t\t 4.6743059158325195\n",
      "\n",
      "Nº de lote:\t 70\n",
      "Loss:\t\t\t 42.91216278076172\n",
      "Reconstruction_loss:\t 38.275794982910156\n",
      "KL_loss:\t\t 4.636368274688721\n",
      "\n",
      "Nº de lote:\t 80\n",
      "Loss:\t\t\t 41.308815002441406\n",
      "Reconstruction_loss:\t 36.706459045410156\n",
      "KL_loss:\t\t 4.602356433868408\n",
      "\n",
      "Nº de lote:\t 90\n",
      "Loss:\t\t\t 42.576026916503906\n",
      "Reconstruction_loss:\t 38.04750061035156\n",
      "KL_loss:\t\t 4.5285186767578125\n",
      "\n",
      "\n",
      "\tAccuracy/Loss Promedio\n",
      "\t\tEntrenamiento: 41.792893\n",
      "\t\t Validación: 41.129875\n",
      "Itenración: 2 / 10 -----------------------------\n",
      "Nº de lote:\t 0\n",
      "Loss:\t\t\t 41.53691482543945\n",
      "Reconstruction_loss:\t 36.8994255065918\n",
      "KL_loss:\t\t 4.637490272521973\n",
      "\n",
      "Nº de lote:\t 10\n",
      "Loss:\t\t\t 39.94084548950195\n",
      "Reconstruction_loss:\t 35.245399475097656\n",
      "KL_loss:\t\t 4.695438861846924\n",
      "\n",
      "Nº de lote:\t 20\n",
      "Loss:\t\t\t 41.212650299072266\n",
      "Reconstruction_loss:\t 36.59657287597656\n",
      "KL_loss:\t\t 4.616077423095703\n",
      "\n",
      "Nº de lote:\t 30\n",
      "Loss:\t\t\t 41.08087921142578\n",
      "Reconstruction_loss:\t 36.44200134277344\n",
      "KL_loss:\t\t 4.638878345489502\n",
      "\n",
      "Nº de lote:\t 40\n",
      "Loss:\t\t\t 40.89307403564453\n",
      "Reconstruction_loss:\t 36.27293014526367\n",
      "KL_loss:\t\t 4.620143890380859\n",
      "\n",
      "Nº de lote:\t 50\n",
      "Loss:\t\t\t 41.514156341552734\n",
      "Reconstruction_loss:\t 36.815242767333984\n",
      "KL_loss:\t\t 4.698916435241699\n",
      "\n",
      "Nº de lote:\t 60\n",
      "Loss:\t\t\t 40.767662048339844\n",
      "Reconstruction_loss:\t 36.080833435058594\n",
      "KL_loss:\t\t 4.686829090118408\n",
      "\n",
      "Nº de lote:\t 70\n",
      "Loss:\t\t\t 40.244895935058594\n",
      "Reconstruction_loss:\t 35.61491775512695\n",
      "KL_loss:\t\t 4.629979610443115\n",
      "\n",
      "Nº de lote:\t 80\n",
      "Loss:\t\t\t 46.07137680053711\n",
      "Reconstruction_loss:\t 41.3582763671875\n",
      "KL_loss:\t\t 4.71310567855835\n",
      "\n",
      "Nº de lote:\t 90\n",
      "Loss:\t\t\t 42.10565185546875\n",
      "Reconstruction_loss:\t 37.404937744140625\n",
      "KL_loss:\t\t 4.700707912445068\n",
      "\n",
      "\n",
      "\tAccuracy/Loss Promedio\n",
      "\t\tEntrenamiento: 41.544257\n",
      "\t\t Validación: 41.082151\n",
      "Itenración: 3 / 10 -----------------------------\n",
      "Nº de lote:\t 0\n",
      "Loss:\t\t\t 39.91396713256836\n",
      "Reconstruction_loss:\t 35.16905212402344\n",
      "KL_loss:\t\t 4.744913578033447\n",
      "\n",
      "Nº de lote:\t 10\n",
      "Loss:\t\t\t 40.24845504760742\n",
      "Reconstruction_loss:\t 35.582191467285156\n",
      "KL_loss:\t\t 4.66626501083374\n",
      "\n",
      "Nº de lote:\t 20\n",
      "Loss:\t\t\t 40.31957244873047\n",
      "Reconstruction_loss:\t 35.571937561035156\n",
      "KL_loss:\t\t 4.747629642486572\n",
      "\n",
      "Nº de lote:\t 30\n",
      "Loss:\t\t\t 41.1579475402832\n",
      "Reconstruction_loss:\t 36.536888122558594\n",
      "KL_loss:\t\t 4.6210618019104\n",
      "\n",
      "Nº de lote:\t 40\n",
      "Loss:\t\t\t 42.47456359863281\n",
      "Reconstruction_loss:\t 37.752525329589844\n",
      "KL_loss:\t\t 4.722040176391602\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[112], line 13\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mItenración: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m(epoch\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m / \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m -----------------------------\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m#Train\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m output_output \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_loop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43mVAE_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43mvae_loss\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m training_losses[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m output_output[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m#Validation\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[80], line 30\u001b[0m, in \u001b[0;36mtrain_loop\u001b[0;34m(loader, model, optimizer, loss_fn)\u001b[0m\n\u001b[1;32m     26\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()             \u001b[38;5;66;03m#Calculo de gradiente\u001b[39;00m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# Save Losses\u001b[39;00m\n\u001b[1;32m     29\u001b[0m losses[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mappend(\n\u001b[0;32m---> 30\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     31\u001b[0m losses[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreconstruction_loss\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mappend(\n\u001b[1;32m     32\u001b[0m     reconstruction_loss\u001b[38;5;241m.\u001b[39mmean()\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m     33\u001b[0m )\n\u001b[1;32m     34\u001b[0m losses[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mKL_loss\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mappend(\n\u001b[1;32m     35\u001b[0m     KL_loss\u001b[38;5;241m.\u001b[39mmean()\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m     36\u001b[0m )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "training_losses = {\n",
    "        \"loss\": [],\n",
    "        \"reconstruction_loss\": [],\n",
    "        \"KL_loss\": []\n",
    "    }\n",
    "val_losses = []\n",
    "\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Itenración: {(epoch + 1)} / {epochs} -----------------------------\")\n",
    "    \n",
    "    #Train\n",
    "    output_output = train_loop(train_loader,VAE_model,optimizer,vae_loss)\n",
    "    training_losses[\"loss\"] += output_output[\"loss\"]\n",
    "    \n",
    "    #Validation\n",
    "    val_output = val_loop(train_loader,VAE_model,vae_loss)\n",
    "    val_losses += val_output\n",
    "\n",
    "print(\"Finalizado entrenamiento del modelo!\")\n",
    "# output_losses = train_loop(train_loader,VAE_model,optimizer,vae_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "960"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_losses[\"loss\"] += output_losses[\"loss\"]\n",
    "len(train_losses[\"loss\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
