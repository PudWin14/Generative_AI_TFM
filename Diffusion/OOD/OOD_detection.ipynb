{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "76d5fad0",
   "metadata": {},
   "source": [
    "## Detección de Out-of-Distribution (OOD)\n",
    "La comprobación de los elementos fuera de distribución, es decir, de imágenes que no pertenezcan al dataset utilizado, es un elemento crucial para el despliegue de sistemas de Machine-Learning, concretamente, los sistemas no supervisados."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a87fa880",
   "metadata": {},
   "source": [
    "En este código, vamos a implementar la metodología del paper \"Denoising diffusion models for out-of-distribution detection\" para la detección de estas imágenes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6374428",
   "metadata": {},
   "source": [
    "# Importación de Librerías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e25dc635",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn import functional as F\n",
    "from diffusers import PNDMScheduler, UNet2DModel\n",
    "from diffusers import schedulers\n",
    "# from PIL import Image\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from datasets import load_dataset\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0f20e65",
   "metadata": {},
   "source": [
    "# Funciones auxiliares necesarias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4e63c961",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_images(images_tensor:torch.tensor, cmap = \"grey\"):\n",
    "    n_dims = images_tensor.dim()\n",
    "    \n",
    "    if n_dims in (2,3):\n",
    "        x_cat = images_tensor\n",
    "\n",
    "    elif n_dims == 4:\n",
    "        x_list = [img for img in images_tensor]\n",
    "        x_cat = torch.cat(x_list,dim=2)\n",
    "    \n",
    "    else:\n",
    "        raise SyntaxError(\"The dimensions of images_tensor must be between 2 and 4\")\n",
    "\n",
    "    if n_dims != 2:\n",
    "        if x_cat.shape[0] == 1:\n",
    "            plt.imshow(x_cat.movedim(0,-1),cmap);\n",
    "        else:\n",
    "            plt.imshow(x_cat.movedim(0,-1));\n",
    "    else:\n",
    "        plt.imshow(x_cat,cmap);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "23e33fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_images_list(images_list:list[torch.Tensor], cmap = \"grey\") -> None:\n",
    "\n",
    "    images_tensor = torch.concat(images_list,dim=0)\n",
    "\n",
    "    # return images_tensor\n",
    "    # show_images(images_tensor.unsqueeze(1))\n",
    "    show_images(images_tensor.unsqueeze(1),cmap)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbb65f65",
   "metadata": {},
   "source": [
    "# Dataset\n",
    "Usaremos fashionMNIST como dataset de datos En Distribución (IOD) y sus versiones Volteadas como dataset Fuera de Distribución (OOD)\n",
    "\n",
    "De momento, solamente vamos a entrenar a la red, por lo que no necesitaremos las distribuciones OOD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c21108d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess = transforms.Compose([\n",
    "    transforms.ToTensor(),              #To Torch Tensor\n",
    "    transforms.Pad(2),                  # Add a padding of 2 pixels\n",
    "    transforms.Normalize([0.5], [0.5])  # Normalize to (-1,1)\n",
    "])\n",
    "\n",
    "def dataset_preprocess(examples):\n",
    "    images = [preprocess(example) for example in examples[\"image\"]]\n",
    "    return {\"images\": images}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "a08366d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"fashion_mnist\")\n",
    "\n",
    "_,test_dataset = torch.utils.data.random_split(dataset[\"train\"].with_transform(dataset_preprocess),(0.8,0.2))\n",
    "\n",
    "# val_dataset = torch.utils.data.random_split(dataset[\"test\"].with_transform(dataset_preprocess),(1,))[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "5ea0d188",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_dataloder = torch.utils.data.DataLoader(\n",
    "#     train_dataset,\n",
    "#     shuffle=True,\n",
    "#     batch_size=256\n",
    "# )\n",
    "IOD_dataloder = torch.utils.data.DataLoader(\n",
    "    test_dataset,\n",
    "    shuffle=True,\n",
    "    batch_size=256\n",
    ")\n",
    "# val_dataloder = torch.utils.data.DataLoader(\n",
    "#     val_dataset,\n",
    "#     shuffle=True,\n",
    "#     batch_size=256\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "278e44f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "OOD_preprocess = transforms.Compose([\n",
    "    transforms.ToTensor(),              #To Torch Tensor\n",
    "    transforms.RandomVerticalFlip(1),   # Vertical flip all the images\n",
    "    transforms.Pad(2),                  # Add a padding of 2 pixels\n",
    "    transforms.Normalize([0.5], [0.5])  # Normalize to (-1,1)\n",
    "])\n",
    "\n",
    "def dataset_OOD_preprocess(examples):\n",
    "    images = [OOD_preprocess(example) for example in examples[\"image\"]]\n",
    "    return {\"images\": images}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "1df82a5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "OOD_dataset,_ = torch.utils.data.random_split(dataset[\"train\"].with_transform(dataset_OOD_preprocess),(0.5,0.5))\n",
    "\n",
    "OOD_dataloader = torch.utils.data.DataLoader(\n",
    "    OOD_dataset,\n",
    "    shuffle=True,\n",
    "    batch_size=256\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "feaaf4ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_images(next(iter(OOD_dataloader))[\"images\"][5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8874fb2e",
   "metadata": {},
   "source": [
    "Vamos a leer los datos de la distribución formada por las métricas para poder realizar el cálculo de los z-scores más adelante"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "5699c893",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "MSE_distribution_list = []\n",
    "LPIPS_distribution_list = []\n",
    "\n",
    "with open(\"Base_data_distribution.csv\",\"r\") as file:\n",
    "\n",
    "    reader = csv.reader(file)\n",
    "\n",
    "    for row in reader:\n",
    "        if row[0] == \"MSE_distribution\":\n",
    "            MSE_distribution_list.append(tuple([float(item) for item in row[1:]]))\n",
    "        \n",
    "        elif row[0] == \"LPIPS_distribution\":\n",
    "            LPIPS_distribution_list.append(tuple([float(item) for item in row[1:]]))\n",
    "\n",
    "        else:\n",
    "            raise ValueError(\"No se encuentra el nombre del tipo de dato en la primera columna\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "984ed59a",
   "metadata": {},
   "source": [
    "# Modelo\n",
    "Importamos el Modelo del fichero de pesos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "904a8dc4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "UNet2DModel(\n",
       "  (conv_in): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (time_proj): Timesteps()\n",
       "  (time_embedding): TimestepEmbedding(\n",
       "    (linear_1): Linear(in_features=32, out_features=128, bias=True)\n",
       "    (act): SiLU()\n",
       "    (linear_2): Linear(in_features=128, out_features=128, bias=True)\n",
       "  )\n",
       "  (down_blocks): ModuleList(\n",
       "    (0): DownBlock2D(\n",
       "      (resnets): ModuleList(\n",
       "        (0-1): 2 x ResnetBlock2D(\n",
       "          (norm1): GroupNorm(32, 32, eps=1e-05, affine=True)\n",
       "          (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (time_emb_proj): Linear(in_features=128, out_features=32, bias=True)\n",
       "          (norm2): GroupNorm(32, 32, eps=1e-05, affine=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (nonlinearity): SiLU()\n",
       "        )\n",
       "      )\n",
       "      (downsamplers): ModuleList(\n",
       "        (0): Downsample2D(\n",
       "          (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (1): DownBlock2D(\n",
       "      (resnets): ModuleList(\n",
       "        (0): ResnetBlock2D(\n",
       "          (norm1): GroupNorm(32, 32, eps=1e-05, affine=True)\n",
       "          (conv1): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (time_emb_proj): Linear(in_features=128, out_features=64, bias=True)\n",
       "          (norm2): GroupNorm(32, 64, eps=1e-05, affine=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (nonlinearity): SiLU()\n",
       "          (conv_shortcut): Conv2d(32, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (1): ResnetBlock2D(\n",
       "          (norm1): GroupNorm(32, 64, eps=1e-05, affine=True)\n",
       "          (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (time_emb_proj): Linear(in_features=128, out_features=64, bias=True)\n",
       "          (norm2): GroupNorm(32, 64, eps=1e-05, affine=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (nonlinearity): SiLU()\n",
       "        )\n",
       "      )\n",
       "      (downsamplers): ModuleList(\n",
       "        (0): Downsample2D(\n",
       "          (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (2): AttnDownBlock2D(\n",
       "      (attentions): ModuleList(\n",
       "        (0-1): 2 x Attention(\n",
       "          (group_norm): GroupNorm(32, 128, eps=1e-05, affine=True)\n",
       "          (to_q): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (to_k): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (to_v): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (to_out): ModuleList(\n",
       "            (0): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (1): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (resnets): ModuleList(\n",
       "        (0): ResnetBlock2D(\n",
       "          (norm1): GroupNorm(32, 64, eps=1e-05, affine=True)\n",
       "          (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (time_emb_proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (norm2): GroupNorm(32, 128, eps=1e-05, affine=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (nonlinearity): SiLU()\n",
       "          (conv_shortcut): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (1): ResnetBlock2D(\n",
       "          (norm1): GroupNorm(32, 128, eps=1e-05, affine=True)\n",
       "          (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (time_emb_proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (norm2): GroupNorm(32, 128, eps=1e-05, affine=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (nonlinearity): SiLU()\n",
       "        )\n",
       "      )\n",
       "      (downsamplers): ModuleList(\n",
       "        (0): Downsample2D(\n",
       "          (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (3): AttnDownBlock2D(\n",
       "      (attentions): ModuleList(\n",
       "        (0-1): 2 x Attention(\n",
       "          (group_norm): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
       "          (to_q): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (to_k): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (to_v): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (to_out): ModuleList(\n",
       "            (0): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (1): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (resnets): ModuleList(\n",
       "        (0): ResnetBlock2D(\n",
       "          (norm1): GroupNorm(32, 128, eps=1e-05, affine=True)\n",
       "          (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (time_emb_proj): Linear(in_features=128, out_features=256, bias=True)\n",
       "          (norm2): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (nonlinearity): SiLU()\n",
       "          (conv_shortcut): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (1): ResnetBlock2D(\n",
       "          (norm1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
       "          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (time_emb_proj): Linear(in_features=128, out_features=256, bias=True)\n",
       "          (norm2): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (nonlinearity): SiLU()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (up_blocks): ModuleList(\n",
       "    (0): AttnUpBlock2D(\n",
       "      (attentions): ModuleList(\n",
       "        (0-2): 3 x Attention(\n",
       "          (group_norm): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
       "          (to_q): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (to_k): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (to_v): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (to_out): ModuleList(\n",
       "            (0): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (1): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (resnets): ModuleList(\n",
       "        (0-1): 2 x ResnetBlock2D(\n",
       "          (norm1): GroupNorm(32, 512, eps=1e-05, affine=True)\n",
       "          (conv1): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (time_emb_proj): Linear(in_features=128, out_features=256, bias=True)\n",
       "          (norm2): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (nonlinearity): SiLU()\n",
       "          (conv_shortcut): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (2): ResnetBlock2D(\n",
       "          (norm1): GroupNorm(32, 384, eps=1e-05, affine=True)\n",
       "          (conv1): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (time_emb_proj): Linear(in_features=128, out_features=256, bias=True)\n",
       "          (norm2): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (nonlinearity): SiLU()\n",
       "          (conv_shortcut): Conv2d(384, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (upsamplers): ModuleList(\n",
       "        (0): Upsample2D(\n",
       "          (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (1): AttnUpBlock2D(\n",
       "      (attentions): ModuleList(\n",
       "        (0-2): 3 x Attention(\n",
       "          (group_norm): GroupNorm(32, 128, eps=1e-05, affine=True)\n",
       "          (to_q): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (to_k): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (to_v): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (to_out): ModuleList(\n",
       "            (0): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (1): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (resnets): ModuleList(\n",
       "        (0): ResnetBlock2D(\n",
       "          (norm1): GroupNorm(32, 384, eps=1e-05, affine=True)\n",
       "          (conv1): Conv2d(384, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (time_emb_proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (norm2): GroupNorm(32, 128, eps=1e-05, affine=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (nonlinearity): SiLU()\n",
       "          (conv_shortcut): Conv2d(384, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (1): ResnetBlock2D(\n",
       "          (norm1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
       "          (conv1): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (time_emb_proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (norm2): GroupNorm(32, 128, eps=1e-05, affine=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (nonlinearity): SiLU()\n",
       "          (conv_shortcut): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (2): ResnetBlock2D(\n",
       "          (norm1): GroupNorm(32, 192, eps=1e-05, affine=True)\n",
       "          (conv1): Conv2d(192, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (time_emb_proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (norm2): GroupNorm(32, 128, eps=1e-05, affine=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (nonlinearity): SiLU()\n",
       "          (conv_shortcut): Conv2d(192, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (upsamplers): ModuleList(\n",
       "        (0): Upsample2D(\n",
       "          (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (2): UpBlock2D(\n",
       "      (resnets): ModuleList(\n",
       "        (0): ResnetBlock2D(\n",
       "          (norm1): GroupNorm(32, 192, eps=1e-05, affine=True)\n",
       "          (conv1): Conv2d(192, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (time_emb_proj): Linear(in_features=128, out_features=64, bias=True)\n",
       "          (norm2): GroupNorm(32, 64, eps=1e-05, affine=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (nonlinearity): SiLU()\n",
       "          (conv_shortcut): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (1): ResnetBlock2D(\n",
       "          (norm1): GroupNorm(32, 128, eps=1e-05, affine=True)\n",
       "          (conv1): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (time_emb_proj): Linear(in_features=128, out_features=64, bias=True)\n",
       "          (norm2): GroupNorm(32, 64, eps=1e-05, affine=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (nonlinearity): SiLU()\n",
       "          (conv_shortcut): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (2): ResnetBlock2D(\n",
       "          (norm1): GroupNorm(32, 96, eps=1e-05, affine=True)\n",
       "          (conv1): Conv2d(96, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (time_emb_proj): Linear(in_features=128, out_features=64, bias=True)\n",
       "          (norm2): GroupNorm(32, 64, eps=1e-05, affine=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (nonlinearity): SiLU()\n",
       "          (conv_shortcut): Conv2d(96, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (upsamplers): ModuleList(\n",
       "        (0): Upsample2D(\n",
       "          (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (3): UpBlock2D(\n",
       "      (resnets): ModuleList(\n",
       "        (0): ResnetBlock2D(\n",
       "          (norm1): GroupNorm(32, 96, eps=1e-05, affine=True)\n",
       "          (conv1): Conv2d(96, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (time_emb_proj): Linear(in_features=128, out_features=32, bias=True)\n",
       "          (norm2): GroupNorm(32, 32, eps=1e-05, affine=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (nonlinearity): SiLU()\n",
       "          (conv_shortcut): Conv2d(96, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (1-2): 2 x ResnetBlock2D(\n",
       "          (norm1): GroupNorm(32, 64, eps=1e-05, affine=True)\n",
       "          (conv1): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (time_emb_proj): Linear(in_features=128, out_features=32, bias=True)\n",
       "          (norm2): GroupNorm(32, 32, eps=1e-05, affine=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (nonlinearity): SiLU()\n",
       "          (conv_shortcut): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (mid_block): UNetMidBlock2D(\n",
       "    (attentions): ModuleList(\n",
       "      (0): Attention(\n",
       "        (group_norm): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
       "        (to_q): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (to_k): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (to_v): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (to_out): ModuleList(\n",
       "          (0): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (1): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (resnets): ModuleList(\n",
       "      (0-1): 2 x ResnetBlock2D(\n",
       "        (norm1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
       "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (time_emb_proj): Linear(in_features=128, out_features=256, bias=True)\n",
       "        (norm2): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (nonlinearity): SiLU()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (conv_norm_out): GroupNorm(32, 32, eps=1e-05, affine=True)\n",
       "  (conv_act): SiLU()\n",
       "  (conv_out): Conv2d(32, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_model = UNet2DModel(\n",
    "    in_channels=1,  # 1 channels for grey scale\n",
    "    out_channels=1,\n",
    "    sample_size=32,  # Specify our input size\n",
    "    # The number of channels per block affects the model size\n",
    "    block_out_channels=(32, 64, 128, 256),\n",
    "    down_block_types=(\n",
    "        \"DownBlock2D\",\n",
    "        \"DownBlock2D\",\n",
    "        \"AttnDownBlock2D\",\n",
    "        \"AttnDownBlock2D\",\n",
    "    ),\n",
    "    up_block_types=(\n",
    "        \"AttnUpBlock2D\",\n",
    "        \"AttnUpBlock2D\",\n",
    "        \"UpBlock2D\",\n",
    "        \"UpBlock2D\"\n",
    "        ),\n",
    "\n",
    ").cuda()\n",
    "\n",
    "base_model.load_state_dict(torch.load(\"Base_model_OOD_detection.pth\",weights_only=True))\n",
    "base_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3b10c55c",
   "metadata": {},
   "outputs": [],
   "source": [
    "scheduler = PNDMScheduler(\n",
    "    num_train_timesteps=1000, beta_start=0.0015, beta_end=0.0195\n",
    ")\n",
    "scheduler.set_timesteps(50)    # Especificamos el nº de pasos de inferencia que usaremos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4146dcf",
   "metadata": {},
   "source": [
    "# Generación de imágenes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cc4b9a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def PNDM_generation_loop(input_img:torch.Tensor, input_timestep : int, model: UNet2DModel, scheduler : PNDMScheduler):\n",
    "\n",
    "    if input_img.dim() != 4:     # Control de Errores\n",
    "        raise SyntaxError(\"Error de Dimensiones. El Tensor de entrada dbe tener 4 dimensions, siendo la primera la dimensión de lote\")\n",
    "    \n",
    "    noisy_x = input_img\n",
    "\n",
    "    if input_timestep < 0 or input_timestep > 1000: # Control de Errores\n",
    "        raise SyntaxError(\"El timestep debe estar entre 0 y 1000\")\n",
    "    \n",
    "    if input_timestep == 1000:  # Si el Timestep es de 1000, se genera una imagen desde cero\n",
    "        idx = 0\n",
    "    else:                       # Si no es de 1000, se comienza desde el punto correspondiente, con los datos de la imagen deseada\n",
    "        idx = torch.where(scheduler.timesteps == input_timestep)[0][0]  # Buscamos el indice del timestep en la lista del scheduler\n",
    "\n",
    "    for t in scheduler.timesteps[idx:]:     # Iteramos sobre la lista del scheduler. Cada elemento es uno de los timesteps de la cadena\n",
    "\n",
    "        with torch.inference_mode():        # Realizamos un paso de la iteración\n",
    "            noise_pred = model(noisy_x, t,return_dict=False)[0]\n",
    "\n",
    "        scheduler_output = scheduler.step(noise_pred, t, noisy_x)   # Paso del scheduler\n",
    "\n",
    "        noisy_x = scheduler_output.prev_sample                      # Realimentamos el bucle\n",
    "    \n",
    "    return(scheduler_output.prev_sample)  # Devolvemos el resultado."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36e43d8c",
   "metadata": {},
   "source": [
    "# Métricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "c23bb646",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchmetrics.functional import mean_squared_error as MSE\n",
    "from torchmetrics.functional.image.lpips import learned_perceptual_image_patch_similarity as LPIPS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfff3191",
   "metadata": {},
   "source": [
    "# Detección de OODs\n",
    "1. Obtenemos una imagen del dataset de OOD (FashionMNIST volteado verticalmente)\n",
    "2. Realizamos las N restauraciones de la imagen.\n",
    "    * Generamos ruido gaussiano\n",
    "    * Añadimos el ruido según los timesteps dados\n",
    "    * Realizamos la restauración con nuestra red\n",
    "    * Calculamos las métricas\n",
    "3. Calculamos el z-score con los datos recopilados anteriormente\n",
    "4. Promediamos los z-scores\n",
    "\n",
    "5. Comparamos los z-scores para averiguar si la imagen pertenece o no al dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "769adeba",
   "metadata": {},
   "outputs": [],
   "source": [
    "OOD_imgs = next(iter(OOD_dataloader))[\"images\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f459275f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
