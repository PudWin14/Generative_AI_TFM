{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff2dce23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn import functional as F\n",
    "from diffusers import PNDMScheduler, UNet2DModel\n",
    "# from diffusers import schedulers\n",
    "# from PIL import Image\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from datasets import load_dataset\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df19e1f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_images(images_tensor:torch.tensor, cmap = \"grey\"):\n",
    "    n_dims = images_tensor.dim()\n",
    "    \n",
    "    if n_dims in (2,3):\n",
    "        x_cat = images_tensor\n",
    "\n",
    "    elif n_dims == 4:\n",
    "        x_list = [img for img in images_tensor]\n",
    "        x_cat = torch.cat(x_list,dim=2)\n",
    "    \n",
    "    else:\n",
    "        raise SyntaxError(\"The dimensions of images_tensor must be between 2 and 4\")\n",
    "\n",
    "    if n_dims != 2:\n",
    "        if x_cat.shape[0] == 1:\n",
    "            plt.imshow(x_cat.movedim(0,-1),cmap);\n",
    "        else:\n",
    "            plt.imshow(x_cat.movedim(0,-1));\n",
    "    else:\n",
    "        plt.imshow(x_cat,cmap);\n",
    "def show_images_list(images_list:list[torch.Tensor], cmap = \"grey\") -> None:\n",
    "\n",
    "    images_tensor = torch.concat(images_list,dim=0)\n",
    "\n",
    "    # return images_tensor\n",
    "    # show_images(images_tensor.unsqueeze(1))\n",
    "    show_images(images_tensor.unsqueeze(1),cmap)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd785cfb",
   "metadata": {},
   "source": [
    "# Cargar el Dataset de Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a56ddcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess = transforms.Compose([\n",
    "    transforms.ToTensor(),              #To Torch Tensor\n",
    "    transforms.Pad(2),                  # Add a padding of 2 pixels\n",
    "    transforms.Normalize([0.5], [0.5])  # Normalize to (-1,1)\n",
    "])\n",
    "\n",
    "def dataset_preprocess(examples):\n",
    "    images = [preprocess(example) for example in examples[\"image\"]]\n",
    "    return {\"images\": images}\n",
    "dataset = load_dataset(\"fashion_mnist\")\n",
    "\n",
    "# train_dataset,test_dataset = torch.utils.data.random_split(dataset[\"train\"].with_transform(dataset_preprocess),(0.8,0.2))\n",
    "\n",
    "val_dataset = torch.utils.data.random_split(dataset[\"test\"].with_transform(dataset_preprocess),(1,))[0]\n",
    "\n",
    "\n",
    "# Solamente necesitamos los datos de validaciónS\n",
    "val_dataloder = torch.utils.data.DataLoader(\n",
    "    val_dataset,\n",
    "    shuffle=True,\n",
    "    batch_size=32       # Tamaño del lote de 32 imágenes. El dataset de Validación son 10.000 imágenes -> 313 lotes\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d02014a9",
   "metadata": {},
   "source": [
    "# Modelo\n",
    "Importamos el Modelo del fichero de pesos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2954131e",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = UNet2DModel(\n",
    "    in_channels=1,  # 1 channels for grey scale\n",
    "    out_channels=1,\n",
    "    sample_size=32,  # Specify our input size\n",
    "    # The number of channels per block affects the model size\n",
    "    block_out_channels=(32, 64, 128, 256),\n",
    "    down_block_types=(\n",
    "        \"DownBlock2D\",\n",
    "        \"DownBlock2D\",\n",
    "        \"AttnDownBlock2D\",\n",
    "        \"AttnDownBlock2D\",\n",
    "    ),\n",
    "    up_block_types=(\n",
    "        \"AttnUpBlock2D\",\n",
    "        \"AttnUpBlock2D\",\n",
    "        \"UpBlock2D\",\n",
    "        \"UpBlock2D\"\n",
    "        ),\n",
    "\n",
    ").cuda()\n",
    "\n",
    "base_model.load_state_dict(torch.load(\"Base_model_OOD_detection.pth\",weights_only=True))\n",
    "base_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72048202",
   "metadata": {},
   "outputs": [],
   "source": [
    "scheduler = PNDMScheduler(\n",
    "    num_train_timesteps=1000, beta_start=0.0015, beta_end=0.0195\n",
    ")\n",
    "scheduler.set_timesteps(50)    # Especificamos el nº de pasos de inferencia que usaremos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43ff75a5",
   "metadata": {},
   "source": [
    "# Generación de imágenes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70fba615",
   "metadata": {},
   "outputs": [],
   "source": [
    "def PNDM_generation_loop(input_img:torch.Tensor, input_timestep : int, model: UNet2DModel, scheduler : PNDMScheduler):\n",
    "\n",
    "    if input_img.dim() != 4:     # Control de Errores\n",
    "        raise SyntaxError(\"Error de Dimensiones. El Tensor de entrada dbe tener 4 dimensions, siendo la primera la dimensión de lote\")\n",
    "    \n",
    "    noisy_x = input_img\n",
    "\n",
    "    if input_timestep < 0 or input_timestep > 1000: # Control de Errores\n",
    "        raise SyntaxError(\"El timestep debe estar entre 0 y 1000\")\n",
    "    \n",
    "    if input_timestep == 1000:  # Si el Timestep es de 1000, se genera una imagen desde cero\n",
    "        idx = 0\n",
    "    else:                       # Si no es de 1000, se comienza desde el punto correspondiente, con los datos de la imagen deseada\n",
    "        idx = torch.where(scheduler.timesteps == input_timestep)[0][0]  # Buscamos el indice del timestep en la lista del scheduler\n",
    "\n",
    "    for t in scheduler.timesteps[idx:]:     # Iteramos sobre la lista del scheduler. Cada elemento es uno de los timesteps de la cadena\n",
    "\n",
    "        with torch.inference_mode():        # Realizamos un paso de la iteración\n",
    "            noise_pred = model(noisy_x, t,return_dict=False)[0]\n",
    "\n",
    "        scheduler_output = scheduler.step(noise_pred, t, noisy_x)   # Paso del scheduler\n",
    "\n",
    "        noisy_x = scheduler_output.prev_sample                      # Realimentamos el bucle\n",
    "    \n",
    "    return(scheduler_output.prev_sample)  # Devolvemos el resultado."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ce1b115",
   "metadata": {},
   "source": [
    "# Métricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c166a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchmetrics.functional import mean_squared_error as MSE\n",
    "from torchmetrics.functional.image.lpips import learned_perceptual_image_patch_similarity as LPIPS\n",
    "\n",
    "# print(MSE(regeneration,img))\n",
    "# print(LPIPS(regeneration,img))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf9b9517",
   "metadata": {},
   "source": [
    "# Obtención de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a98be52",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 50      # Número de Reconstrucciones\n",
    "reconstructions_timesteps = torch.arange(1000,0,-1000/N)\n",
    "print(reconstructions_timesteps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99f97dfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "MSE_data = [[] for _ in range(N)]\n",
    "LPIPS_data = [[] for _ in range(N)]\n",
    "\n",
    "n_epochs = len(val_dataloder)\n",
    "\n",
    "for epoch,batch in enumerate(val_dataloder):\n",
    "    imgs = batch[\"images\"].cuda()\n",
    "    print(f\"Época {epoch}/{n_epochs}\")\n",
    "    print(\"====================\")\n",
    "    print()\n",
    "\n",
    "    for idx,t in enumerate(reconstructions_timesteps):      # Tiempo estimado: 4h 20min\n",
    "        noise = torch.randn_like(imgs)\n",
    "\n",
    "        if t == 1000:\n",
    "            restoration = PNDM_generation_loop(noise,t,base_model,scheduler)\n",
    "            \n",
    "        else:\n",
    "            noisy_img = scheduler.add_noise(imgs,noise,t.int())\n",
    "            restoration = PNDM_generation_loop(noisy_img,t,base_model,scheduler)\n",
    "\n",
    "        MSE_data[idx].append(MSE(restoration,imgs))\n",
    "        \n",
    "        max_rest,min_rest = restoration.max(),restoration.min()\n",
    "        n_restoration =  2* ( ((restoration-min_rest) / (max_rest-min_rest)) - 0.5)\n",
    "\n",
    "        max_imgs,min_imgs = imgs.max(),imgs.min()\n",
    "        n_imgs =  2* ( ((imgs-min_imgs) / (max_imgs-min_imgs)) - 0.5)\n",
    "\n",
    "\n",
    "        LPIPS_data[idx].append(LPIPS(n_restoration.repeat(1,3,1,1),n_imgs.repeat(1,3,1,1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "581cb4bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv                      # Guardar datos eb fichero para analizar más adelante\n",
    "\n",
    "file_name = \"Base_data.csv\"\n",
    "\n",
    "with open(file_name,\"w\",newline='') as csv_file:\n",
    "    writer = csv.writer(csv_file)\n",
    "\n",
    "    for list in MSE_data:\n",
    "        writer.writerow([\"MSE_data\"] + list)\n",
    "\n",
    "    for list in LPIPS_data:\n",
    "        writer.writerow([\"LPIPS_data\"] + list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24d9428c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e182ab0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a44831ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13e02516",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "716982a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# img = next(iter(val_dataloder))[\"images\"].cuda()    # Elegimos un lote de imágenes\n",
    "# print(len(img)) # Batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bcf3847",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show_images(img[0].cpu())   # Mostramos una de las imágenes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "421216ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# noise = torch.randn_like(img)   # Generamos el Ruido\n",
    "# noise.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c949763",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rand_timestep_idx = torch.randint(0,len(timesteps_list),(1,))               # Elegimos el timestep aleatorio de la lista del Scheduler\n",
    "# rand_timestep = timesteps_list[rand_timestep_idx]\n",
    "\n",
    "# rand_timestep_tensor = torch.ones((val_dataloder.batch_size,),dtype=int)*rand_timestep\n",
    "# rand_timestep_tensor[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bae12bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# noisy_img = scheduler.add_noise(img,noise,rand_timestep_tensor)         # Añadimos Ruido a las imágenes\n",
    "# show_images(noisy_img[0].cpu())     # Mostramos una imagen con ruido"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a630262b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# regeneration = PNDM_generation_loop(noisy_img,rand_timestep,base_model,scheduler)       # Realizamos la Restauración"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13fb99cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show_images(regeneration[10:20].cpu())  # Mostramos las imágenes restauradas\n",
    "# show_images(regeneration.cpu())  # Mostramos las imágenes restauradas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "221d0c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show_images(img[10:20].cpu())   # Mostramos las imágenes originales\n",
    "# show_images(img.cpu())   # Mostramos las imágenes originales\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c904b4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MSE_list = []\n",
    "# for idx in range(val_dataloder.batch_size):\n",
    "#     MSE_list.append(MSE(regeneration[idx],img[idx]).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f777a00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import statistics\n",
    "# statistics.mean(MSE_list)\n",
    "# MSE_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a3206a8",
   "metadata": {},
   "source": [
    "Pasos a seguir para obtener las distribuciones base:\n",
    "1. Obtengo una imagen\n",
    "2. Se le añaden 50 cantidades distintas de ruido.\n",
    "3. Para cada cantidad de ruido, se realiza la reconstrucción de la imagen\n",
    "4. Se calculan las métricas: MSE y LPIPS\n",
    "\n",
    "5. Tras la obtención de los datos, calculamos su distribución: Media y desviación estándar"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
