{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b38b37f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import diffusers\n",
    "import matplotlib.pyplot as plt\n",
    "from datasets import load_dataset\n",
    "from torchvision import transforms\n",
    "import kagglehub\n",
    "from pandas import read_csv, concat\n",
    "from pandas import DataFrame\n",
    "\n",
    "from PIL import Image\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8149fd2",
   "metadata": {},
   "source": [
    "## Data\n",
    "Download, transform and import images from dataset to program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3f49e7e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Looks like you're using an outdated `kagglehub` version (installed: 0.3.8), please consider upgrading to the latest version (0.3.11).\n"
     ]
    }
   ],
   "source": [
    "# Import the Dataset from kaggle and load on a Tensor\n",
    "Nist_dataset = kagglehub.dataset_download(\"sachinpatel21/az-handwritten-alphabets-in-csv-format\")\n",
    "\n",
    "chunksize = 250\n",
    "\n",
    "# the list that contains all the dataframes\n",
    "list_of_dataframes = []\n",
    "\n",
    "for df in read_csv(Nist_dataset+\"/A_Z Handwritten Data.csv\", chunksize=chunksize):\n",
    "    # process your data frame here\n",
    "    # then add the current data frame into the list\n",
    "    list_of_dataframes.append(df)\n",
    "\n",
    "# if you want all the dataframes together, here it is\n",
    "handwritten_alphabet = concat(list_of_dataframes)\n",
    "\n",
    "\n",
    "\n",
    "# handwritten_alphabet = read_csv(Nist_dataset+\"/A_Z Handwritten Data.csv\")\n",
    "# print(handwritten_alphabet)\n",
    "# handwritten_alphabet = torch.Tensor(handwritten_alphabet.values).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "56f3643a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class HandwrittenAlphabetDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, Alphabet_dataframe:DataFrame, img_size: int, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_file (string): Ruta al archivo CSV.\n",
    "            transform (callable, optional): Transformación opcional para aplicar a la imagen.\n",
    "        \"\"\"\n",
    "        self.data = Alphabet_dataframe\n",
    "        self.transform = transform\n",
    "        self.img_size = img_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        # Obtener la etiqueta y los píxeles\n",
    "        label = self.data.iloc[idx, 0]\n",
    "        pixels = self.data.iloc[idx, 1:].values.astype(np.uint8)\n",
    "\n",
    "        # Reformar los píxeles en una imagen (ej. 28x28)\n",
    "        img = pixels.reshape(self.img_size, self.img_size)\n",
    "        img = Image.fromarray(img) # Convertir a objeto PIL Image\n",
    "\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        return img, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "658cea6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size = 28\n",
    "\n",
    "#Prepare images to load to the net\n",
    "\n",
    "preprocess = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize((image_size,image_size)),\n",
    "        transforms.ToTensor(),              #Convert to Tensor\n",
    "        transforms.Normalize([0.5],[0.5])   #Map tp -1 to 1\n",
    "    ]\n",
    ")\n",
    "\n",
    "def transform(examples):\n",
    "    examples = preprocess(examples)\n",
    "    return examples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "b45d30c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and Split the dataset on Train, Validation and Test Datasets\n",
    "dataset = HandwrittenAlphabetDataset(handwritten_alphabet, 28, transform)\n",
    "\n",
    "train,val,test = torch.utils.data.random_split(dataset,[0.8,0.1,0.1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d80e3ead",
   "metadata": {},
   "source": [
    "### A little fnct to show easily images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "2ebb36e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_images(images_tensor:torch.tensor):\n",
    "    n_dims = images_tensor.dim()\n",
    "    \n",
    "    if n_dims in (2,3):\n",
    "        x_cat = images_tensor\n",
    "\n",
    "    elif n_dims == 4:\n",
    "        x_list = [img for img in images_tensor]\n",
    "        x_cat = torch.cat(x_list,dim=2)\n",
    "    \n",
    "    else:\n",
    "        raise SyntaxError(\"The dimensions of images_tensor must be between 2 and 4\")\n",
    "\n",
    "    if n_dims != 2:\n",
    "        if x_cat.shape[0] == 1:\n",
    "            plt.imshow(x_cat.movedim(0,-1),cmap=\"gray\");\n",
    "        else:\n",
    "            plt.imshow(x_cat.movedim(0,-1));\n",
    "    else:\n",
    "        plt.imshow(x_cat,cmap=\"gray\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e038c2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Tensor"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mEl kernel se bloqueó al ejecutar código en la celda actual o en una celda anterior. \n",
      "\u001b[1;31mRevise el código de las celdas para identificar una posible causa del error. \n",
      "\u001b[1;31mHaga clic <a href='https://aka.ms/vscodeJupyterKernelCrash'>aquí</a> para obtener más información. \n",
      "\u001b[1;31mVea Jupyter <a href='command:jupyter.viewOutput'>log</a> para obtener más detalles."
     ]
    }
   ],
   "source": [
    "type(train[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "629c55e7",
   "metadata": {},
   "source": [
    "## Preparing images to the net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "545c45e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#   Create the Dataloader\n",
    "batch_size = 16\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "    train,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "val_dataloader = torch.utils.data.DataLoader(\n",
    "    val,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
